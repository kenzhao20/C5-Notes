\part{Climate Dynamics}\label{Climate Dynamics}

\section*{Introduction}

This section of the course was lectured by \href{https://www.physics.ox.ac.uk/our-people/allenm}{Myles Allen} covering a sort of miscellaneous set of topics. His stated goal (at time of writing) is to introduce to you certain methods using climate as an example. \vspace{5 mm}

\noindent This section consists of three chapters:\vspace{5 mm}

\begin{enumerate}
    \item \hyperref[Dynamical Systems]{Dynamical Systems}: 
        
        \begin{quote}
            We consider the Earth as a simple, low-dimensional, linear, deterministic dynamical system. We solve the system using normal modes and a further method (not discussed by Myles) using the quasi-steady approximation.

            In many cases we find this to reveal multiple timescales of response: a shorter timescale response (where the slower component stays constant) and longer timescale response (where the faster component is always in time-varying equilibrium).
        \end{quote}

    \item \hyperref[Predictability]{Predictability}: 
    
        \begin{quote}
            We discuss the growth of errors and perturbations in a linearised $N$ dimensional dynamical system, and how to minimise such errors in, for example, a weather forecast.

            We briefly discuss the growth of non-linear errors.
        \end{quote}
    
    \item \hyperref[Estimation]{Estimation}:
        
        \begin{quote}
            We introduce the maximum likelihood estimator, the estimator that minimises the squared difference between the prediction and the data (this is just finding a line of best fit).

            We discuss errors.
        \end{quote}
\end{enumerate}

\chapter{Dynamical Systems}\label{Dynamical Systems}

\section{The Equations of Motion}

We first consider the climate as a simple dynamical system, the simplest of which is a linear dynamical deterministic system.

In reality, the climate is highly non-linear and stochastic. As such, we should consider this model as representing only small perturbations about some reference state. If one considers small enough perturbations, any (well-behaved) state will be linear (think: Taylor expansions). The state will, in general, be given by:
\begin{equation}\label{Dynamical System Eqn}
    \dot{\vec{x}}=\hat{J}\vec{x}+\vec{f}
\end{equation}
where $\vec{x}\in\mathbb{R}^n$ is the state-vector of the system, $\hat{J}\in\mathbb{R}^n\times\mathbb{R}^n$ is the linear(ised) Jacobian, and $\vec{f}\in\mathbb{R}^n$ is the external forcing. 

We apply this first to an energy-budget equation for the climate, characterised by the state-vector $(T_s,T_d)$ representing perturbations of the mean atmospheric/surface-ocean temperature and deep ocean temperature, respectively, governed by the equations:
\begin{align}
    \label{T_s}
    &\boxed{C_s \dot{T}_s = F_{ext}(t)-\lambda T_s -\gamma (T_s-T_d) - \lambda'(T_s-T_d)}
    \\ 
    \label{T_d}
    &\boxed{C_d \dot{T}_d = -\gamma (T_d-T_s)}
\end{align}
where $C_s$, $C_d$ are the effective heat capacities of the atmosphere/surface oceans and deep oceans per unit area, respectively. The left-hand side represents the change in the energy (per unit area) and the right-hand side represents various energy fluxes (per unit area). The various terms are explained below:
\begin{figure}[H]
\begin{tabular}{|p{2.8cm}|p{13.4cm}|}
\hline
    Term & Interpretation \\
\hline
\hline
$F_{ext}(t)$ & The perturbation in external forcing due to albedo, aerosol, greenhouse gas, solar, etc. forcing.\\
\hline
$\lambda T_s$ & The additional energy radiated to space (per unit area) due to warming. If $\lambda T_s>0$ then \textit{more} energy is being radiated into space, which typically results in cooling. $\lambda$ is the \textit{climate sensitivity parameter}. \\ 
\hline
$-\gamma (T_s-T_d)$ & The energy flux from the atmosphere/surface oceans \textit{into} the deep oceans. This is typically small, due to the fact that the oceans are very stably stratified. As such, mixing/convection is severely inhibited, and surface waters can only penetrate into the deep oceans at a select few areas (cf. Sec.\ref{MOC}).\\
\hline
$- \lambda'(T_s-T_d)$ & The additional energy radiated into space due to the system being out of equilibrium. We have reason to believe (empirically, due to climate models) that this occurs. We encode this crudely by letting $(T_s-T_d)$ represent the degree to which the climate is out of equilibrium.\\
\hline
\end{tabular}
\end{figure}
Note that if we add Equations \ref{T_s} and \ref{T_d}, we do not get $0$ on the right hand side: the climate is not an isolated system. There is a net energy flux from $F_{ext}$, $-\lambda T_s$, and $-\lambda ' (T_s-T_d)$.

\section{Solving the System by Diagonalising the Matrix}

Before you read this section, I should note that, while this method is completely general to linear systems, it is a very time-consuming method to do in the exam. Furthermore, I think more physical intuition comes from another method of solving the system which I explain in Section \ref{Hewitt Method}. If you're low on time, and the question asks you to sketch it, it might be easier to think in terms of the method in Section \ref{Hewitt Method}.

This method relies on some prior knowledge of linear algebra, specifically diagonalisation, eigenvectors, and eigenvalues. I'll review them quickly here, but if you're familiar with the content you can skip to Section \ref{App to Dym}. 

\subsection{Deriving the Method (Review of ODEs and Linear Algebra)}
\subsubsection{Reframing the ODE Problem into a Linear Algebra Problem}
First we write Equations \ref{T_s} and \ref{T_d} in matrix form:
\begin{align}
    \label{T Matrix}
    \underbrace{\left( \begin{array}{c}
        \dot{T}_s \\\\ \dot{T}_d
    \end{array} 
    \right)}_{\dot{\vec{T}}}
    =
    \underbrace{\left( \begin{array}{cc}
        -\frac{\lambda+\gamma+\lambda'}{C_s} & \frac{\gamma+\lambda'}{C_s} \\\\
        \frac{\gamma}{C_d} & -\frac{\gamma}{C_d}
    \end{array}
    \right)}_{\hat{J}}
    \underbrace{\left(  \begin{array}{c}
        T_s \\\\ T_d
    \end{array}
    \right)}_{\vec{T}}
    +
    \underbrace{\left( \begin{array}{c}
        F_{ext}(t)/C_s \\\\ 0
    \end{array}
    \right)}_{\vec{f}}
\end{align}
We now attempt to \textbf{diagonalise} $\hat{J}$. We find \textbf{eigenvectors} $\vec{v}_i=$ that satisfy the following relation:
\begin{align}
    \label{Eigenequation}
    \hat{J}\vec{v}_i=\lambda_i\vec{v}_i
\end{align}
where $\lambda_i\in\mathbb{R}$ is an \textbf{eigenvalue}. I'll confine our attention to 2D systems here for brevity, but everything I say is readily extendable to $N$ dimensions. In 2D, we write $\vec{v}_i=([\vec{v}_i]_1,[\vec{v}_i]_2)^T$.\footnote{Note further that each eigenvector is non-unique â€“ it is specified only up to a constant of proportionality. I.e., if $\vec{v}_i$ is an eigenvector, then $\alpha\vec{v}_i$ is also an eigenvector for all $\alpha \in \mathbb{R}$.}

We now slap our eigenvectors $\vec{v_i}$ together into a matrix $\hat{E}$ defined as follows:x
\begin{align*}
    \hat{E} = \left( 
        \begin{array}{cc}
            \uparrow & \uparrow \\
            \vec{v}_1 & \vec{v}_2 \\
            \downarrow & \downarrow
        \end{array}
    \right)
\end{align*}
We then define $\hat{\Sigma}=\hat{E}^{-1}\hat{J}\hat{E}$ and show then the following:
\begin{align*}
    \hat{E}^{-1}\hat{J}\hat{E} &= \hat{E}^{-1}\hat{J}\left( 
        \begin{array}{cc}
            \uparrow & \uparrow \\
            \vec{v}_1 & \vec{v}_2 \\
            \downarrow & \downarrow
        \end{array}
    \right)\\
    &=\hat{E}^{-1}\left( 
        \begin{array}{cc}
            \uparrow & \uparrow \\
            \lambda_1\vec{v}_1 & \lambda_2\vec{v}_2 \\
            \downarrow & \downarrow
        \end{array}
    \right)\\
    &=\left( 
        \begin{array}{cc}
            \lambda_1 & 0 \\
            0 & \lambda_2
        \end{array}
    \right)
    \hat{E}^{-1}\hat{E}\\
    \hat{\Sigma}&=\left( 
        \begin{array}{cc}
            \lambda_1 & 0 \\
            0 & \lambda_2
        \end{array}
    \right)
\end{align*}
Therefore, multiplying on the left by $\hat{E}$ and right by $\hat{E}^{-1}$:
\begin{align*}
    \hat{J}=\hat{E}\left( 
        \begin{array}{cc}
            \lambda_1 & 0 \\
            0 & \lambda_2
        \end{array}
    \right)\hat{E}^{-1}
\end{align*}
We can then multiple Equation \ref{T Matrix} on the left by $\hat{E}^{-1}$ and define $\vec{S}\equiv\hat{E}^{-1}\vec{T}$ and $\vec{g}\equiv\hat{E}^{-1}\vec{f}$:
\begin{align*}
    \hat{E}^{-1}\dot{\vec{T}}
    &=
    \hat{E}^{-1}\hat{J}\vec{T}+\hat{E}^{-1}\vec{f}\\
    &=\bcancel{\hat{E}}^{-1}\bcancel{\hat{E}}\hat{\Sigma}\hat{E}^{-1}\vec{T}+\hat{E}^{-1}\vec{f}\\
    \dot{\vec{S}}&=\hat{\Sigma}\vec{S}+\vec{g}
\end{align*}
Recall that $\hat{\Sigma}$ is completely diagonal, so our coupled equations are now uncoupled in the new coordinate system $\vec{S}$! We write the $i$th component of the vectors with a subscript $i$ and thus write that:
\begin{align*}
    \dot{S}_i=\lambda_i S_i + g_i
\end{align*}
Therefore, using an integrating factor, our solution is:
\begin{align*}
    \label{Dym System Gen Sol}
    S_i(t)=S_i(0)e^{\lambda_i t}+e^{\lambda_i t}\int_{0}^{t}g_i(t')e^{-\lambda_i t'}\,dt'
\end{align*}
Physically, $S_i(t)$ is the $i$th normal mode, a mode that is decoupled completely from the other modes: this mode will evolve completely ignorant to the behaviour of the other modes. The eigenvectors $\lambda_i$, which have dimensions of inverse-time, represent the timescales of the $i$th normal mode $\tau_i\sim 1/\lambda_i$.

Our final step is to relate $S_i$ back to $T_i$, which is easy if we note that $\vec{S}\equiv\hat{E}^{-1}\vec{T}$, therefore $\vec{T}=\hat{E}\vec{S}$. Therefore, $T_i=[v_i]_1S_1+[v_i]_2S_2$ so our general solution is:
\begin{align}
    \BOX{
        T_j=\sum_{i} \left( [v_j]_i \left( S_i(0)e^{\lambda_i t}+e^{\lambda_i t}\int_{0}^{t}g_i(t')e^{-\lambda_i t'}\,dt' \right) \right)
    }
\end{align}
where $g_i(t)=\sum_{j}[\hat{E}^{-1}]_{ij}[\vec{f}]_j$. You'll have to calculate this explicitly for the problem at hand.

So we can solve our ODE if we find the \textbf{eigenvectors} and \textbf{eigenvalues} of $\hat{J}$! We've transformed our problem into a linear algebra problem! So how do we solve this now?

\subsubsection{Solving the Linear Algebra Problem}

We rewrite Equation \ref{Eigenequation} as follows:
\begin{align*}
    \hat{J}\vec{v}_i&=\lambda_i\vec{v}_i\\
    &=\lambda_i\left( \begin{array}{cc}
        1 & 0 \\
        0 & 1
    \end{array} \right)\vec{v}_i\\
    \therefore \vec{0} & = \left( 
        \vec{J} - \lambda_i \left( \begin{array}{cc}
        1 & 0 \\
        0 & 1
    \end{array} \right)
    \right)\vec{v}_i
\end{align*}
where $\vec{0}=(0,0)^T$. 

Now clearly, an easy solution is if we set $\vec{v}_i=\vec{0}$ and $\lambda_i=0$, but this is only one solution, and we wish to find the non-trivial solutions. One can show in linear algebra that a non-zero matrix times a non-zero vector can be equal to $\vec{0}$ only if the determinant of the matrix is zero. We thus solve for:
\begin{align*}
    \det\left( 
        \vec{J} - \lambda_i \left( \begin{array}{cc}
        1 & 0 \\
        0 & 1
    \end{array} \right)
    \right)=0
\end{align*}
to obtain our eigenvalues $\lambda_i$. Once we have done that we can go through each $\lambda_i$ one at a time and substitute the $\lambda_i$ into Equation \ref{Eigenequation} to find the corresponding $\vec{v}_i$. And we're done!

\subsection{Application to Our Dynamical System}\label{App to Dym}

Before we apply this method to our dynamical system, we make an important simplification. Typically, $C_s\ll C_d$, representing the massive heat capacity of the deep oceans compared to the atmosphere and land. If we write $\hat{J}$ as:
\begin{align*}
    \hat{J}=\left( \begin{array}{cc}
        -\frac{\lambda+\gamma+\lambda'}{C_s} & \frac{\gamma+\lambda'}{C_s} \\\\
        \frac{\gamma}{C_d} & -\frac{\gamma}{C_d}
    \end{array}
    \right)
    =\left( \begin{array}{cc}
        -a & b \\\\
        c & -c
    \end{array}
    \right)
\end{align*}
and assume further that $\lambda$, $\gamma$, and $\lambda'$ are of similar size, we can conclude that $|a|,|b|\gg |c|$. 

Let us now continue with our method. We want to find the eigenvalues of $\hat{J}$. Let us refer to the eigenvalues as $r_i$ to avoid confusion. We want to solve:
\begin{align*}
    \det\left( \begin{array}{cc}
        -a-r_i & b \\\\
        c & -c-r_i
    \end{array}
    \right)=0
\end{align*}
This is a straightforward quadratic in $r_i$, which has the solution:
\begin{align*}
    r_i=\frac{1}{2}\left( -a-c \pm \sqrt{a^2-2ac+4bc+c^2} \right)
\end{align*}
We now keep only first order in $\frac{c}{a}$, and write:
\begin{align*}
    r_i&=\frac{1}{2}\left( -a-c \pm \sqrt{a^2-2ac+4bc+c^2} \right)\\
    &= \frac{1}{2}\left( 
        -a-c \pm a\sqrt{1-\frac{2c}{a}+\frac{4bc}{a^2}+\left( \frac{c}{a} \right)^2}
     \right) \\ 
    &\approx \frac{1}{2}\left( 
        -a-c \pm a\left(1-\frac{c}{a}+\frac{2bc}{a^2}\right)
     \right) \\ 
    r_2&\approx c\left( -1+\frac{b}{a} \right)\hspace{2mm},\hspace{2mm}r_1\approx -a-\frac{bc}{a}
\end{align*}
Recall again that $c\ll a,b$. The eigenvalue on the left $r_2$ is of order $c$, so it is small, but note both terms are of order $c$. As such, we cannot neglect any terms in $r_2$. The eigenvector on the right $r_1$ has the second $-bc/a$ term much smaller than the first $-a$ term, so we can neglect the second term. Our eigenvectors $r_i$ are therefore:
\begin{align*}
    r_1\approx-a\hspace{2mm},\hspace{2mm} r_2\approx c\left(-1 +\frac{b}{a}\right)
\end{align*}
Plugging in values for $a,b,c$, we get:
\begin{align*}
    r_1\approx-\frac{\lambda+\gamma+\lambda'}{C_s}\hspace{2mm},\hspace{2mm}
    r_2\approx-\frac{\lambda\gamma}{\left( \lambda+\gamma+\lambda' \right)C_s}
\end{align*}

Having found the (approximate) eigenvalues, our goal now is to find the (approximate) eigenvectors. We can estimate this directly by substituting in the eigenvalues directly into Equation \ref{Eigenequation} and solving for $\vec{v}_i$. Following some algebra, we find that the corresponding eigenvectors are:
\begin{align*}
    \vec{v}_1= \left( \begin{array}{c}
        1  \\
        0
    \end{array} \right)
    \hspace{2mm},\hspace{2mm}
    \vec{v}_2= \left( \begin{array}{c}
        1  \\
        a/b
    \end{array} \right)
    = \left( \begin{array}{c}
        1  \\
        \left( \lambda+\gamma+\lambda' \right)/\left(\gamma+\lambda' \right)
    \end{array} \right)
\end{align*}

\begin{fact}{Dynamical Systems Recipe}{eigenvectors box}\label{eigenvectors box}
    Suppose we have a coupled set of ODEs governing the time-evolution of $x(t)$ and $y(t)$ given by:
    \begin{align*}
        A \dot{x}= Cx + Dy + G(t)\\
        B \dot{y} = Ex + Fy + H(t)
    \end{align*}
    Solve this system in the following way:
    \begin{enumerate}
        \item Write your ODE in matrix form:
            \begin{align*}
                \dot{\vec{x}}=\hat{J}\vec{x}+\vec{f} \hspace{5 mm}\text{where} \hspace{5 mm}\hat{J} = \left( \begin{array}{cc}
                    C/A & D/A \\
                    E/B & F/B
                \end{array} \right)
                \hspace{2 mm}\text{and} \hspace{2 mm}
                \vec{f}(t)=\left( \begin{array}{c}
                    G(t)/A \\
                    H(t)/B
                \end{array} \right)
            \end{align*}
        \item Find the eigenvectors and eigenvalues of $\hat{J}$.
        \item See if you can exploit any separation of scales to neglect terms. For example, if $A\ll B$, then we can keep only first order in $E/B$ and $F/B$. 
        \item Plug the approximate eigenvalues and eigenvectors into \ref{Dym System Gen Sol}.
    \end{enumerate}
\end{fact}

Note though that it's not as simple as just plugging the eigenvalues and eigenvectors into \ref{Dym System Gen Sol}. We also need to find $g_i(t)$, which means we need to invert the matrix of eigenvalues $\hat{E}$ to find $\hat{E}^{-1}$. This is incredibly time consuming, especially for an exam (but it is possible!), so I'd recommend reading up on this next unofficial method.

\section{Solving the System by Separating Timescales}\label{Hewitt Method}

Here we present an alternative method, and have \href{https://people.maths.ox.ac.uk/hewitt/}{Ian Hewitt} from the Maths department to thank for this way of thinking. 

This method is both less general than the former method, as it requires the assumption that $C_s\ll C_d$, and more general than the former method, as it does \textit{not} require the ODEs to be linear. We present this method because we believe first that this method provides more physical intuition than the previous method, and second that this method is far more time-efficient than the former method, and that most equations presented in this course will be simple enough for this method to save a lot of time, especially in the exam if you have to sketch the general behaviour.

However I should emphasise that this is an `\textbf{unofficial}' method. I cannot guarantee that the marker will understand what you are doing if you do this in the exam.

As already mentioned, we must exploit the separation of timescales here. Here, $T_s$ evolves much faster than $T_d$, which allows us to make two approximations.\footnote{A full rigorous treatment requires non-dimensionalising the equations, but we believe this is ultimately not needed for physical intuition in this case.}

The first approximation we make is valid on \textit{short} time-scales. The terms on the right hand side of equation \ref{T_s} and \ref{T_d} are of both around the same size, therefore $C_s\dot T_s$ and $C_d\dot T_d$ are around the same size. However, since $C_d\gg C_s$, we require that $\dot T_d\ll \dot T_s$. Are first approximation thus involves letting $\dot T_d\approx0$, and we integrate this to find that $T_d(t)=0$ for all times. Physically, this represents the fact that $T_d$ does not respond on very short timescales due to the massive heat capacity $C_d$. Equations \ref{T_s} and \ref{T_d} thus reduce to:
\begin{align}
    \label{T_s short}
    C_s\dot T_s &=F_{ext}(t)-(\lambda +\gamma+ \lambda')T_s\\ 
    \label{T_d short}
    C_d \dot{T_d} &= 0
\end{align}
on short timescales. We can integrate Equation \ref{T_s short} using an integrating factor, with the general solution in Section \ref{Dynamical Systems Solutions}. The timescale is set by comparing the $T_s$ term to the $\dot T_s$ term: $\tau_s\sim \frac{C_s}{\lambda+\lambda'+\gamma}=1/r_1$. Thus on an $O(\tau_s)$ timescale, $T_s$ equilibrates and $T_d$ remains constant (at $0$). We have derived our first eigenvalue ($1/\tau_s$) and eigenvector ($(1,0)^T$) here without diagonalising anything!

The second approximation we make is valid on \textit{long} time-scales. Here we consider when $T_d$ is of the same size as $T_s$. For this to be the case, the terms on the right hand side of equation \ref{T_d} must be massive (because $C_d$) is massive, and so the terms on the right hand side of equation \ref{T_s} must be massive, and so $C_s \dot T_s$ must be massive as well. We anticipate that under such conditions, $T_s$ will evolve rapidly into equilibrium, as the only way to balance such a system must be to allow $C_s \dot T_s=0$. On this timescale, we say that $T_s$ is \textbf{quasi-steady}: $T_s$ evolves rapidly into equilibrium with $T_d$, and so we set $\dot T_s=0$ in Equation \ref{T_s}. Equations \ref{T_s} and \ref{T_d} thus reduce to:
\begin{align}
    0 & = F_{ext}(t)-\lambda T_s -\gamma (T_s-T_d) - \lambda'(T_s-T_d) \label{T_s long} \\ 
    C_d \dot{T_d} &= -\gamma (T_d-T_s) \label{T_d long}
\end{align}
on long timescales. We now solve for $T_s(t)$ in terms of $T_d(t)$ and substitute $T_s(t)$ into Equation \ref{T_d}. Solving Equation \ref{T_s long} gives $T_s(t)=\frac{1}{\lambda+\gamma+\lambda'}\left(F_{ext}(t)+(\gamma+\lambda ')T_d(t)\right)$. Substituting this into \ref{T_d long} gives the following ODE governing $T_d$:
\begin{align}
    C_d \dot{T_d} &= \frac{\gamma}{\lambda+\gamma+\lambda'}F_{ext}(t) - \frac{\gamma\lambda}{\lambda+\gamma+\lambda'}T_d\label{T_d long T_s} 
\end{align}
on long timescales. The timescale is set again by comparing the $T_d$ term to the $\dot{T_d}$ term: $\tau_d\sim \frac{C_d(\lambda+\gamma+\lambda')}{\gamma\lambda}=1/r_2$. Again, we have derived our second eigenvalue ($1/\tau_d$) and eigenvector () here without diagonalising anything!

We can now say, in retrospect, that the short timescale approximation (Eqns. \ref{T_s short} and \ref{T_d short}) is valid on timescales much shorter than $\tau_d$, that the long timescale approximation (Eqns. \ref{T_s long} and \ref{T_d long}) is valid on timescales much longer than $\tau_s$, and that we require $\tau_d\gg \tau_s$. This is satisfied if and only if $\frac{C_d(\lambda+\gamma+\lambda')}{\gamma\lambda}\gg\frac{C_s}{\lambda+\lambda'+\gamma}$, which is indeed satisfied if $\gamma,\lambda,\lambda'$ are of similar size (which they are) and $C_d\gg C_s$, as originally assumed.

Note the nuance in these two approximations, where we confusingly seem to set the time derivatives to $0$ in both cases. In the first case, we set $\dot T_d=0$ because the timescales are too \textit{short}, and $T_d$ essentially does not react on this timescale. In the second case, we set $\dot T_s=0$ because the timescales are too \textit{long}. The timescales are so long that we assume that $\dot T_s$ rapidly equilibrates such that at each time, it is always in equilibrium, and thus $\dot T_s=0$

\begin{fact}{Timescale Approximations}{timescale box}\label{timescale box}
    Suppose we have a coupled set of ODEs governing the time-evolution of $x(t)$ and $y(t)$ given by:
    \begin{align*}
        A \dot{x}=f(x,y)\\
        B \dot{y} = g(x,y)
    \end{align*}
    such that $B\gg A$ and $f$ and $g$ are of similar size. We further assume that $x(t=0)=y(t=0)=0$. Then we can make two approximations which indicate the different responses:

    \begin{minipage}{.5\linewidth}
        \begin{tcolorbox}[colback=myyellow!50!white,colframe=mymagenta]
            The \textbf{Transient Response}: On \textbf{short} timescales, $\tau_s$, we assume that $y$ has no time to respond so that:
            \begin{gather*}
                \dot{y}=0
            \end{gather*}
            Therefore the transient response is governed by:
            \begin{align}
                \label{1 transient}
                \boxed{A\dot{x}=f(x,0)}\\
                \boxed{y=0}
            \end{align}
            We find the timescale $\tau_s$ by comparing the $\dot{x}$ term to the $x$ term in Equation \ref{1 transient}.
        \end{tcolorbox}
    \end{minipage}
    \begin{minipage}{.5\linewidth}
        \begin{tcolorbox}[colback=myyellow!50!white,colframe=mymagenta]
            The \textbf{Quasi-Steady Response}: On \textbf{long} timescales, $\tau_d$, we assume that $x$ responds rapidly into equilibrium so that:
            \begin{gather*}
                f(x,y)=0
            \end{gather*}
            Therefore the transient response is governed by:
            \begin{align}
                \boxed{0=f(x,y)}\\
                \label{2 quasi}
                \boxed{B\dot{y}=g(x(y),y)}
            \end{align}
            We find the timescale $\tau_d$ by comparing the $\dot{y}$ term to the $y$ terms in Equation \ref{2 quasi}.
        \end{tcolorbox}
    \end{minipage}
\end{fact}

\section{Solutions}\label{Dynamical Systems Solutions}
We can use either of the methods above to solve the ODEs. Let us now specify that $\vec{f}(t)=(F_{ext}(t),0)^T$ and assume that $T_s(0)=T_d(0)=0$. In the solution, we get two different responses for an arbitrary $F_{ext}(t)$:\vspace{5 mm}

\noindent On short timescales, i.e., the normal mode with the larger eigenvalue and thus shorter timescale (we define $\tau_s\equiv\frac{C_s}{\lambda+\lambda'+\gamma}$): 
\begin{align}
    T_s(t) & \approx \frac{1}{C_s}\int_0^tF_{ext}(\hat{t}) e^{-\frac{t+\hat{t}}{\tau_s}}d\hat{t} \label{T_s short soln} \\ 
    T_d(t)& \approx 0 \label{T_d short soln}
\end{align}
On long timescales, i.e., the normal mode with the smaller eigenvalue and thus longer timescale (we define $\tau_d\equiv\frac{C_d(\lambda+\lambda'+\gamma)}{\gamma\lambda}$):
\begin{align}
    T_s(t) & \approx \frac{1}{\lambda+\gamma+\lambda'}\left(F_{ext}(t)+(\gamma+\lambda ')T_d(t)\right) \\ 
    T_d(t) & \approx \frac{\gamma}{C_d(\lambda+\gamma+\lambda')}\int_0^tF_{ext}(\hat{t}) e^{-\frac{t+\hat{t}}{\tau_d}}d\hat{t}  \label{T_d long step soln}
\end{align}

Let us solve this system explicitly for some given $F_{ext}(t)$. Suppose $F_{ext}(t)$ is a step-function, representing, for example, an instantaneous emission of CO$_2$ which remains in the atmosphere indefinitely:
\begin{align*}
    F_{ext}=
    \begin{cases}
        0 & \text{if } t<0 \\
        F & \text{if } t>0
    \end{cases}
\end{align*}

Now clearly, if $t<0$, $T_s(t)=T_d(t)=0$. For the values of $t>0$, we integrate explicitly to find the solutions in each case. On a short timescale:
\begin{align*}
    T_s(t) & \approx \frac{1}{C_s}e^{-\frac{t}{\tau_s}}\int_0^t F e^{\frac{\hat{t}}{\tau_s}}d\hat{t}\\
    &=\frac{1}{C_s}e^{-\frac{t}{\tau_s}} \left( F\tau_s
    \left( e^{\frac{t}{\tau_s}} - 1 \right) \right)\\
    &=\frac{F\tau_s}{C_s}\left( 1-e^{-\frac{t}{\tau_s}} \right)
    \\ 
    T_d(t)& \approx 0 
\end{align*}
We find then that on an $O(\tau_s)$ timescale, $T_s$ equilibrates with a timescale of $\tau_s$ to its new equilibrium value, and $T_d$ remains constant. Note how, despite the fact that $T_d=0=const$, the deep oceans are not `invisible'. The impact of the deep oceans is encoded in the timescale $\tau_s$, which I remind you depends on the $\gamma$ parameter, representing the heat flux from the atmosphere/surface-ocean to the deep oceans. The atmosphere/surface-ocean thus equilibrates to a level where the heat flux in (from the increase in $F_{ext}(t)$) equals the heat flux out (from more heat flux into space, and, crucially, more heat flux into the deep oceans).

On long timescales:
\begin{align}
    T_s(t) & \approx \frac{1}{\lambda+\gamma+\lambda'}\left(F+(\gamma+\lambda ')T_d(t)\right) \\ 
    T_d(t) & \approx \frac{\gamma}{C_d(\lambda+\gamma+\lambda')}\int_0^t F e^{-\frac{t+\hat{t}}{\tau_d}}d\hat{t}\\
    &=
\end{align}

[EXAMPLE SOLUTIONS UNDER CONSTRUCTION]

\section{Analogous Systems}

We can apply this to similar systems. Consider, for example, a carbon budget equation, given by:
\begin{align}
    R_a \dot{C_a} &= E(t)-r (C_a-C_b) \\ 
    R_b \dot{C_b} &= r (C_a-C_b) - s (C_b-C_d)\\
    R_d \dot{C_d} &=  s (C_b-C_d)
\end{align}
where $C_a,C_b,C_d$ are the effective CO$_2$ anomalies in the atmosphere, biosphere/surface-ocean, and deep ocean, respectively. The left hand side is the change in CO$_2$ concentrations, and the right hand side are carbon fluxes. It's typically the case that $R_d\gg R_b\gg R_a$ due to similar reasons as before, as well as a bit of ocean chemistry. We can use the exact same method as before to define three timescales:
\begin{enumerate}
    \item A short timescale ($t_a\sim\mathcal{O}\left(\frac{R_a}{r}\right)$):  We set $C_b\approx C_d\approx 0$, as $C_b,C_d$ respond very slowly. $C_a$ evolves according to $R_a \dot{C_a} = E(t)-r C_a$.
    \item A medium timescale ($t_b\sim\mathcal{O}\left(\frac{R_b}{s}\right)$): We set $C_d \approx 0$ since $C_d$ responds very slowly, and we set $\dot{C_a} \approx 0$ as $C_a$ evolves rapidly into equilibrium with $C_b$. We then get that $C_a(t)$ evolves quasi-steadily ($C_a(t)\approx C_b(t)+\frac{E(t)}{r}$) and $C_b$ evolves according to $R_b \dot{C_b}=E(t)-sC_b$.
    \item A long timescale ($t_d\gg\mathcal{O}\left(\frac{R_b}{s}\right)$): We set $\dot{C_a}\approx \dot{C_b} \approx 0$ since $C_a,C_b$ evolve rapidly into equilibrium with $C_d$. We then get that $C_a(t)$ and $C_b(t)$ evolve quasi-steadily ($C_a(t)\approx C_b(t)+\frac{E(t)}{r}$ and $C_b(t)\approx C_d(t) +\frac{E(t)}{s}$), and $C_d$ evolves according to $R_d \dot{C_d}=E(t)$. Note that it is difficult to define a timescale here as from our second method â€“ we cannot simply compare $C_d$ to $\dot{C_d}$. 
    
    If we defer to diagonalising the matrix, we find that the eigenvalue is $0$, and so the timescale is actually $t_d\sim\infty$! This reflects the fact that our carbon budget system is closed, and once you pump in carbon (represented by the $E(t)$ term), it has nowhere to go other than to stay in the system. As such, if $E(t)>0$ for all $t$, the system will never relax to a steady state, and $Ca$, $C_b$, and $C_d$ will all grow without bound.

    If you wish to solve what happens in this system, I would recommend doing the returning quasi-steady method, and solving the single ODE governing $C_d$ (assuming that $C_a$ and $C_b$ are quasisteady). 
\end{enumerate}
This system, again, may be solved by finding eigenvalues and eigenvectors. 

\chapter{Predictability}\label{Predictability}

\section{The Linear Error Propagator}

We continue considering a linear dynamical system here, for example, a weather system. Consider a state characterised by some state-vector $\vec{x}\in\mathbb{R}^n$ (perhaps encoding the pressure, temperature, humidity, and velocity at each point in space\footnote{In reality, of course, such a state-vector would be infinite-dimensional, as the temperature (for example) is a function of position $T(\vec{r},t)$, which is continuous. However, all numerical models must discretise space (i.e., divide space into 'boxes'), reducing $\vec{x}$ to a very large but finite dimensional vector.}), whose time-evolution (or perhaps, linearised time-evolution) is given by:
\begin{align}\label{Jac}
    \dot{\vec{x}} = \hat{J}\vec{x}
\end{align}
where $\hat{J}\in\mathbb{R}^n\times\mathbb{R}^n$ is the Jacobian given by:
\begin{align}
    {\hat{J}}  =  \left( \begin{array}{ccc}
    \frac{\partial \dot{x}_1}{\partial x_1} & \frac{\partial \dot{x}_1}{\partial x_2} & \cdots \\
    \frac{\partial \dot{x}_2}{\partial x_1} & \frac{\partial \dot{x}_2}{\partial x_2} & \cdots \\
    \vdots & \vdots & \vdots \\
    \end{array} \right)
\end{align} 
Given that $\vec{x}$ has $n$-dimensions, we require $n$ initial conditions given by $\vec{x}(t=t_0)$ to integrate this ODE. This will typically achieved via data assimilation, but all that needs to be known at this point is that we cannot know the initial conditions to a sufficient precision, because the system is chaotic.

Therefore, we aim to find how errors in the initial conditions propagate and evolve. Suppose that the actual state of the system is given by $\vec{y}(t)$ and our prediction is given by $\vec{x}(t)$. We assume $\vec{y}$ time-evolves by Equation \ref{Jac}.\footnote{This amounts to the assumption that our model is perfect, i.e., there is no model error. This is, of course, false, so this method is only applicable to dealing with errors arising from errors in initial conditions.} We define some initial error or perturbation\footnote{We use error and perturbation interchangeably in this chapter} $\vec{\delta x}(t)$ such that, at all times, the following holds:
\begin{align}
    \vec{y}(t)=\vec{x}(t)+\vec{\delta x}(t)
\end{align}

At time $t=t_0$ then, our initial error is $\vec{\delta x}(t=t_0)$. We wish to find how this error evolves as we integrate the system forwards.

We know that
\begin{align*}
    \vec{y}(t_0+\delta t) & = \vec{y}(t_0) + \int_{t_0}^{t_0+\delta t} \hat{J} \vec{y}(t') dt'\\
    & = \vec{x}(t_0) + \vec{\delta x}(t_0)+ \int_{t_0}^{t_0+\delta t} \hat{J} (\vec{x}(t') + \vec{\delta x}(t')) dt'\\
    & = \vec{x}(t_0) + \int_{t_0}^{t_0+\delta t} \hat{J} \vec{x}(t') dt'+ \vec{\delta x}(t_0)+ \int_{t_0}^{t_0+\delta t} \hat{J} \vec{\delta x}(t') dt'\\
    & = \vec{x}(t_0+\delta t) + \vec{\delta x}(t_0+\delta t)
\end{align*}
where we have used the definition of $\vec{\delta x}(t)$ and the fact that $\hat{J}$ is a linear(ised) operator. Therefore, 
\begin{align*}
    \vec{\delta x}(t_0+\delta t) & = \vec{\delta x}(t_0)+ \int_{t_0}^{t_0+\delta t} \hat{J} \vec{\delta x}(t') dt'\\
    & \approx \vec{\delta x}(t_0)+ \hat{J} \delta t\vec{\delta x}(t_0)\\
    & = (\mathbb{I} + \hat{J}\delta t)\vec{\delta x}(t_0)
\end{align*}
Where we have used the fact that $\delta t$ is small. We define the linear error propagator $\hat{A}$ as:
\begin{align}
    & \BOX{\hat{A} = (\mathbb{I} + \hat{J}\delta t)}\\
    & \BOX{\vec{\delta x}(t_0+\delta t)\approx\hat{A}\vec{\delta x}(t_0)} \label{dx}
\end{align}

\section{Local Perturbation Growth}

Suppose we want to find which initial errors will grow the most. This may because we want to run an ensemble weather forecast: instead of integrating from a single initial condition, we will integrate our weather model from multiple initial conditions compatible with the observations. Generally, the final states will be different from each other as they depend on the initial conditions, and the final spread will indicate how predictable the current state is or whether there are any low probability high impact weather events. However, we want to prudently choose which initial conditions we integrate from, given our limited computing resources, which is why we want to find which initial errors will grow the most.

Finding which errors will grow the most amounts to trying to trying to maximise $||\vec{\delta x}(t_0+\delta t)||$ with respect to $\vec{\delta x}(t_0)$. This can't be the whole story though, since $\vec{\delta x}(t_0+\delta t)$ is a linear function of $\vec{\delta x}(t_0)$ (Equation \ref{dx}), so we can maximise $\vec{\delta x}(t_0+\delta t)$ simply by letting $\vec{\delta x}(t_0)\to\infty$. More precisely, then, we want to find the \textit{direction} of $\vec{\delta x}(t_0)$ which maximises $||\vec{\delta x}(t_0+\delta t)||$. We can do this by introducing a constraint on the magnitude of the initial error (i.e., enforce $||\vec{\delta x}(t_0)||-C=0,C\in\mathbb{R}$)\footnote{Myles sets $C=1$, but $C$ can actually just be any number.} and maximising $||\vec{\delta x}(t_0+\delta t)||$ using Lagrange multipliers. So we introduce the Lagrange multiplier $\lambda$ and maximise our Lagrangian $\mathscr{L}$.
\begin{align}
    \mathscr{L}=\underbrace{||\vec{\delta x}(t_0+\delta t)||^2}_{\text{function to be maximised}}-\lambda\underbrace{(||\vec{\delta x}(t_0)||^2-C^2)}_\text{constraint}
\end{align}
Substituting in Equation \ref{dx} and differentiating with respect to $\vec{\delta x}(t_0)$, we find that:
\begin{align*}
    \frac{\partial\mathscr{L}}{\partial\vec{\delta x
    }(t_0)}&=2(\hat{A}^T\hat{A}\vec{\delta x}(t_0)-\lambda\vec{\delta x}(t_0))\\
    &=0
\end{align*}
if and only if:
\begin{align*}
    \hat{A}^T\hat{A}\vec{\delta x}(t_0)=\lambda\vec{\delta x}(t_0)
\end{align*}
This is a stationary point only if the initial error $\vec{\delta x}(t_0)$ is an eigenvector of $\hat{A}^T\hat{A}$. However, not all stationary points are maximisers (e.g., it could be a minimiser or a saddle point). It turns out that the final error is maximised only if the initial error is the \textit{largest} eigenvector of $\hat{A}^T\hat{A}$\footnote{
    This is deduced by considering the Hessian: the matrix of second partial derivatives. It can be shown, analogous to the 1D case, that the stationary point is a maximiser only if the Hessian is negative semidefinite (i.e., each eigenvalue $\leq0$). In this case, the hessian is the matrix $\hat{A}^T\hat{A}-\lambda\mathbb{I}$, which is clearly only negative semidefinite if $\lambda\geq$ the largest eigenvalue of $\hat{A}^T\hat{A}$.}
. Note that $\hat{A}^T\hat{A}$ is symmetric, so (by a theorem in linear algebra) eigenvectors will be orthogonal and eigenvalues will be real.

\chapter{Estimation}\label{Estimation}

\section{The Pseudo-Inverse \texorpdfstring{$\hat{K}$}{K-hat}}

Suppose we take $n$ measurements, which we encode in some matrix $\vec{y}\in\mathbb{R}^m$, with some random error $\vec{\epsilon}\in\mathbb{R}^m$, in order to estimate some values $\vec{x}\in\mathbb{R}^n$ where $n<m$. The measurements $\vec{y}$ are some (known) function of $\vec{x}$ like so: $\vec{y}=f(\vec{x})+\vec{\epsilon}$ where $f:\mathbb{R}^n\to\mathbb{R}^m$.

In some cases, $f$ is linear, and we can represent $f(\vec{x})=\hat{H}\vec{x}$ where $\hat{H}\in\mathbb{R}^m\times\mathbb{R}^n$ (i.e., $\hat{H}$ is an $m\times n$ matrix). We know $\vec{y}$, but we wish to estimate $\vec{x}$. Since $m<n$, the system is overdetermined: we cannot simply invert $\hat{H}$ and solve for $\vec{x}$, which is exacerbated by the error $\vec{\epsilon}$.

Instead, we we apply linear regression, and minimise the squared difference between the actual (error-prone) measurements $\vec{y}$ and the predicted measurements $\vec{\hat{y}}=\hat{H}\vec{x}$, where $\vec{\hat{x}}\in\mathbb{R}^m$ is our estimate of $\vec{x}$. (We denote our prediction/estimates by hats and the real values without hats).

One can differentiate $||\vec{y}-\vec{\hat{y}}||^2=||\vec{y}-\hat{H}\vec{\hat{x}}||^2$ with respect to $\vec{\hat{x}}$ (since everything is smooth) and churn through some algebra to obtain the least-squares estimator.
\begin{align}
    \vec{\hat{x}}& =\hat{K}\vec{y} \\
    & = (\hat{H}^T\hat{H})^{-1}\hat{H}^T\vec{y}
\end{align}
where the pseudo-inverse $\hat{K}$ of $\hat{H}$ is:

\begin{align}
    \BOX{\hat{K}= (\hat{H}^T\hat{H})^{-1}\hat{H}^T}
\end{align}

We call $\hat{K}$ the pseudo-inverse of $\hat{H}$ because $\hat{K}\hat{H}=\mathbb{I}$ (but note that generally $\hat{H}\hat{K}\neq\mathbb{I}$). In practice in exams, you will probably have to do this quickly on your calculator. The strategy is as follows: 

\begin{fact}{Estimator Recipe}{estimator box}\label{estimator box}
    Suppose you need to estimate some quantities $x_i$ based on some data $y_j$. Find the \textbf{least-squares estimator} in the following way:
    \begin{enumerate}
        \item Rearrange the system into a linear system as follows:
    \begin{align*}
        \vec{y}=\hat{H}\vec{x}
    \end{align*}
        \item Enter the matrix $\hat{H}$ into your calculator and calculate the \textbf{pseudo-inverse} $\hat{K}= (\hat{H}^T\hat{H})^{-1}\hat{H}^T$. 
        \item Yeet the $\hat{H}$ onto the observations $\vec{y}$ (which you should be given), i.e., estimate $\vec{x}$ as $\vec{\hat{x}}=\hat{K}\vec{y}_0$.
        \item Watch the examiners award you The Gibbs Prize for Performance in the MPhys examination (Â£500).
        \item Donate those Â£500 (and maybe the prize as well) to the author(s) of these lecture notes.
    \end{enumerate}
\end{fact}

\section{Errors}

We assume, for simplicity, that there are negligible errors on the parameters within the operator $\hat{H}$. We assume only that there are some errors $\vec{\epsilon}$ on the observations $\vec{y}_0$. We assume that the errors are given by some well defined $n\times n$ covariance matrix $\hat{S}$:
\begin{align*}
    \braket{\vec{\epsilon}\,\vec{\epsilon}\,^T}=\hat{S}\\
    [\hat{S}]_{ij}=[\vec{\epsilon}]_i[\vec{\epsilon}]_j
\end{align*}
We also assume that the average, indicated by brakets $\braket{}$, of the noise is $0$:
\begin{align*}
    \braket{\vec{\epsilon}}=0
\end{align*}
This allows us to show that, on average:

\begin{align*}
    \braket{\vec{\hat{x}}}&=\braket{\hat{K}\,\vec{y}}
    \\
    &=\braket{\hat{K}\,(\hat{H}\,\vec{x}+\vec{\epsilon})}
    \\
    &=\braket{\vec{x}}
    \\
    \\
    Var\left(\vec{\hat{x}}\right)&
    =\braket{(\vec{x}-\vec{\hat{x}})(\vec{x}-\vec{\hat{x}})^T}
    \\
    &=\braket{(\vec{x}-\hat{K}\,\vec{y})(\vec{x}-\hat{K}\,\vec{y})^T}
    \\
    &
    =\braket{(\vec{x}-\hat{K}\,(\hat{H}\,\vec{x}+\vec{\epsilon}))(\vec{x}-(\hat{H}\,\vec{x}+\vec{\epsilon}))^T}
    \\
    &=\braket{\hat{K}\,\vec{\epsilon}\,\vec{\epsilon}\,^T\hat{K}^T}
    \\
    &=\hat{K}\hat{S}\hat{K}^T
\end{align*}

In other words, the mean of our estimation is the real value, and the covariance of our estimation is the covariance of 

[UNDER CONSTRUCTION]

\section{Interpretation}

[UNDER CONSTRUCTION]

\section{Examples}

[UNDER CONSTRUCTION]